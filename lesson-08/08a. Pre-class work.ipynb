{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7c39dbc",
      "metadata": {
        "id": "d7c39dbc"
      },
      "source": [
        "# Robust linear regression: An exploration\n",
        "\n",
        "In the previous lesson, we saw that an outlier can lead a model with a Normal likelihood function in the wrong direction. Normal distributions have very light tails and any model that uses them needs to compensate for outliers by disproportionately adjusting the values of the Normal mean and variance in the posterior distribution.\n",
        "\n",
        "The goal of robust regression is to either reduce the impact of outliers on the posterior mean and variance or to explicitly detect and identify outliers. Another common practice is to remove outliers manually but _that_ road is fraught with bias. We should always encode our assumptions _in the model_ rather than modifying the data on an ad hoc basis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vuZmTKsY46lg",
      "metadata": {
        "id": "vuZmTKsY46lg"
      },
      "source": [
        "## Prep plan\n",
        "\n",
        "* There is no assigned reading from the textbook today.\n",
        "* You will spend all your prep time on this workbook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac11601a",
      "metadata": {
        "id": "ac11601a"
      },
      "source": [
        "## Workbook outline\n",
        "\n",
        "1. We start by reproducing the results from the last class where we did linear regression with a Normal likelihood function.\n",
        "\n",
        "2. We then explore how a heavy-tailed distribution in the likelihood function changes the inference results.\n",
        "\n",
        "3. Finally, we look at a more complicated model that automatically detects which data are outliers and which are not.\n",
        "\n",
        "**Click the _Run All_ button now** so the workbook produces output while you are reading the information below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a789cd0d",
      "metadata": {
        "id": "a789cd0d"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "As before, this data set shows relates the percentage of adults who do not engage in any physical exercise/activity to the percentage of adults who are obese in different states in the USA. Source: [Huston, J. (2022, January 23). Lab 13 Obesity. Kaggle code.](https://www.kaggle.com/code/joshuahuston/lab-13-obesity/notebook)\n",
        "\n",
        "In order to test how well our models work, we create two copies of the data set — one with and one without the outlier.\n",
        "\n",
        "To be clear, we want to fit our models to the complete data set but we also want that the outlier in the complete data set doesn’t affect our results much. In order to check whether or not the outlier affects our results, we check what happens if we remove the outlier manually.\n",
        "\n",
        "**Run this code cell** and look at the plots of the two copies of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4cdb4d1",
      "metadata": {
        "id": "c4cdb4d1"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import scipy.stats as sts\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    'https://course-resources.minerva.edu/uploaded_files/mu/00265107-3137/obesity.csv')\n",
        "\n",
        "# The complete data set. We sort the data by x-coordinate to make the plots look\n",
        "# better so we know datum number 3, for example, is the 3rd point from the left.\n",
        "# This has no impact on the inference results.\n",
        "data_x = np.array(df['no_activity_perc'])\n",
        "order = np.argsort(data_x)\n",
        "data_x = data_x[order]\n",
        "data_y = np.array(df['obesity_perc'])[order]\n",
        "\n",
        "# Remove the outlier and create a second data set\n",
        "outlier = np.where(data_x > 40)[0][0]\n",
        "data_removed_x = np.concatenate((data_x[:outlier], data_x[outlier+1:]))\n",
        "data_removed_y = np.concatenate((data_y[:outlier], data_y[outlier+1:]))\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Complete data set')\n",
        "plt.xlabel('% adults reporting no leisure physical activity')\n",
        "plt.ylabel('% adults with obesity')\n",
        "plt.plot(data_x, data_y, 'k.', alpha=0.5)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Data set with outlier removed')\n",
        "plt.xlabel('% adults reporting no leisure physical activity')\n",
        "plt.ylabel('% adults with obesity')\n",
        "plt.plot(data_removed_x, data_removed_y, 'k.', alpha=0.5)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4666c3e6",
      "metadata": {
        "id": "4666c3e6"
      },
      "source": [
        "Explain in your own words _how much_ of a difference that one outlier could make to the results. You should discuss why it is an outlier (how do you see that on the plot) and how far away it is from what you would expect of a non-outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f33b0a",
      "metadata": {
        "id": "f5f33b0a"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67818411",
      "metadata": {
        "id": "67818411"
      },
      "source": [
        "## The Normal model (same as last time)\n",
        "\n",
        "We fit the model from the previous lesson with and without the outlier data point.\n",
        "\n",
        "Likelihood:\n",
        "\n",
        "$$y_i \\sim \\text{Normal}(\\mu_i, \\sigma^2)$$\n",
        "\n",
        "$$\\mu_i = c_0 + c_1 x_i$$\n",
        "\n",
        "Prior:\n",
        "\n",
        "$$c_0 \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "$$c_1 \\sim \\text{Normal}(0, 5^2)$$\n",
        "\n",
        "$$\\sigma \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "**Run this code cell** to create the model in PyMC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b0918a",
      "metadata": {
        "id": "d6b0918a"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as normal_model:\n",
        "    # Prior\n",
        "    c0 = pm.Uniform('c0', lower=0, upper=100)\n",
        "    c1 = pm.Normal('c1', mu=0, sigma=5)\n",
        "    sigma = pm.Uniform('sigma', lower=0, upper=100)\n",
        "    # Data\n",
        "    x = pm.Data('x', data_x)\n",
        "    y = pm.Data('y', data_y)\n",
        "    # Regression mean\n",
        "    mu = pm.Deterministic('mu', c0 + c1 * x)\n",
        "    # Likelihood\n",
        "    pm.Normal('likelihood', mu=mu, sigma=sigma, observed=y)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(pm.model_to_graphviz(normal_model).render(format='png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6429c420",
      "metadata": {
        "id": "6429c420"
      },
      "source": [
        "This model has 3 free variables — $c_0$, $c_1$, and $\\sigma$. Explain why we do not count the $x_i$, $y_i$ and $\\mu_i$ variables as _free_ variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cccbc18",
      "metadata": {
        "id": "4cccbc18"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0fc074",
      "metadata": {
        "id": "5a0fc074"
      },
      "source": [
        "### Normal model: Posteriors with all data\n",
        "\n",
        "We run the sampler on the full data set, contained in the variables `data_x` and `data_y`.\n",
        "\n",
        "**Run this code cell** and confirm that the diagnostic output from the sampler looks fine.\n",
        "\n",
        "* The R-hat values should be close to 1.\n",
        "* The effective sample size should be at least a few hundred.\n",
        "* The rank-plot histograms should be approximately uniform.\n",
        "\n",
        "**Note:** We don't display _all_ the posterior variables in the diagnostic output since there are too many of them — there is a $\\mu_i$ parameter for each data point. We just display the linear regression coefficients ($c_0$ and $c_1$) and the noise scale ($\\sigma$). The `var_names` argument is used to specify which variables are shown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28f62f7",
      "metadata": {
        "id": "b28f62f7"
      },
      "outputs": [],
      "source": [
        "with normal_model:\n",
        "    normal_inference_all = pm.sample()\n",
        "\n",
        "az.plot_rank(normal_inference_all, var_names=['c0', 'c1', 'sigma'])\n",
        "az.summary(normal_inference_all, var_names=['c0', 'c1', 'sigma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a7fb76",
      "metadata": {
        "id": "d1a7fb76"
      },
      "source": [
        "### Normal model: Posteriors without the outlier\n",
        "\n",
        "We run the sampler without the outlier datum. Note how the code below uses the `set_data` function to update the values of the PyMC variables `x` and `y`.\n",
        "\n",
        "**Run this code cell** and confirm that the diagnostic output from the sampler looks fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62628263",
      "metadata": {
        "id": "62628263"
      },
      "outputs": [],
      "source": [
        "with normal_model:\n",
        "    pm.set_data({\n",
        "        'x': data_removed_x,\n",
        "        'y': data_removed_y})\n",
        "    normal_inference_without = pm.sample()\n",
        "\n",
        "az.plot_rank(normal_inference_without, var_names=['c0', 'c1', 'sigma'])\n",
        "az.summary(normal_inference_without, var_names=['c0', 'c1', 'sigma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c0ea64",
      "metadata": {
        "id": "13c0ea64"
      },
      "source": [
        "### The difference\n",
        "\n",
        "To see that the posteriors changed quite a lot, we can look at the `az.summary()` output for `c0`, `c1`, and `sigma`. However, it is easier to see on a plot.\n",
        "\n",
        "**Run the code cell below** and interpret the results on the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1dfe867",
      "metadata": {
        "id": "a1dfe867"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.suptitle('Posterior distributions for the two data sets')\n",
        "for i, var in enumerate(['c0', 'c1', 'sigma']):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.xlabel(var)\n",
        "    if i == 0:\n",
        "        plt.ylabel('probability density')\n",
        "    plt.hist(getattr(normal_inference_all.posterior, var).values.flatten(), density=True, bins=50, edgecolor='white', alpha=0.5, label='all data')\n",
        "    plt.hist(getattr(normal_inference_without.posterior, var).values.flatten(), density=True, bins=50, edgecolor='white', alpha=0.5, label='no outlier')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d41514",
      "metadata": {
        "id": "b2d41514"
      },
      "source": [
        "Interpret the results. How much of a difference does the outlier data point make in the posteriors over $c_0$, $c_1$, and $\\sigma$? Is this a big difference or a small difference? Explain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e95263",
      "metadata": {
        "id": "20e95263"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0558b67",
      "metadata": {
        "id": "e0558b67"
      },
      "source": [
        "## Student T linear regression\n",
        "\n",
        "A problem with the Normal distribution in the likelihood function is that its tails are too light to deal with outliers. That means the probability of observing what we usually call an outlier is really, really low according to the Normal likelihood. As a result, the posterior distribution needs to make $\\sigma$ larger to accommodate the outlier. Usually the posterior also needs to adjust $c_0$ and $c_1$ to make the straight line fit tend towards the outlier.\n",
        "\n",
        "Instead of the light-tailed Normal distribution, we can attempt to use a distribution with heavier tails. This is a common approach to what is called _robust_ linear regression — robust here means that our inference about the regression line is not overly influenced by a small number of outliers. Note that this changes the likelihood function for all the data and not just the outlier. A model with a heavy-tailed likelihood basically assumes that the data distribution can produce samples far away from the mean.\n",
        "\n",
        "The T distribution, also known as the Student T distribution, has a parameter called $\\nu$ (pronounced \"new\") that adjusts the lightness of the tails. The larger $\\nu$ is, the lighter the tails are.\n",
        "* When $\\nu=1$, the tails are very heavy and samples that look like outliers are common.\n",
        "* When $\\nu\\approx30$, the T distribution already looks a lot like the Normal distribution and you can prove that for $\\nu\\rightarrow\\infty$, the T distribution converges to the Normal distribution.\n",
        "\n",
        "That's really nice. If there are no outliers in the data, a linear regression model with a T likelihood should give you virtually the same results as a model with a Normal likelihood since $\\nu$ should be large in the posterior. However, if there are outliers, the model should automatically make $\\nu$ small, resulting in heavier tails."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b6dbb8",
      "metadata": {
        "id": "72b6dbb8"
      },
      "source": [
        "### Example: How far away are typical samples from the mean?\n",
        "\n",
        "Here is a quick demonstration of how a heavy-tailed T distribution really is very different from a light-tailed Normal distribution. If we generate 100 samples from a standard Normal distribution and find the maximum, it's rarely more than 3 sigma units away from the mean. For the standard normal, the mean is 0 and $\\sigma=1$.\n",
        "\n",
        "If you run the code cell below, which generates a hundred $\\text{Normal}(0, 1)$ samples, a few times, you should see that the maximum value is usually between 2 and 3 and only rarely more than 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e0a0a2a",
      "metadata": {
        "id": "5e0a0a2a"
      },
      "outputs": [],
      "source": [
        "sts.norm(loc=0, scale=1).rvs(100).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e33dfd31",
      "metadata": {
        "id": "e33dfd31"
      },
      "source": [
        "Let's try that with a T distribution with $\\nu=1$, which is heavy-tailed. It is not uncommon to get samples that are _hundreds_ of sigmas away from the middle. (The probability of seeing a maximum greater than 100 is about 0.28.) If you run the cell lots of times, you might even get a result that is _thousands_ of sigmas from the middle. (The probability of seeing a maximum greater than 1000 is about 0.03).\n",
        "\n",
        "We would never observe such an outcome with the Normal distribution, even if we waited until the heat death of the universe. The probability of seeing a maximum greater than 100 with Normal samples is approximately $10^{-120}$.\n",
        "\n",
        "You should run the code cell below for different values of $\\nu$ (the `df` parameter in the code since $\\nu$ is known as the \"degrees of freedom\" parameter) and confirm that the samples behave much more like Normal samples when $\\nu\\ge30$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c30be68d",
      "metadata": {
        "id": "c30be68d"
      },
      "outputs": [],
      "source": [
        "sts.t.rvs(df=1, loc=0, scale=1, size=100).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f75632",
      "metadata": {
        "id": "12f75632"
      },
      "source": [
        "You can also experiment further with the T distribution using [this Geogebra plot](https://www.geogebra.org/m/za632sqy) or [this interactive plot](https://homepage.divms.uiowa.edu/~mbognar/applets/t.html). Note how wide the 95% interval of the distribution is when $\\nu=1$.\n",
        "\n",
        "You can, of course, also plot the T distribution in Python. The T distribution has a lot more probability mass away from the middle (in the tails)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54683426",
      "metadata": {
        "id": "54683426"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.title(r'PDF of the T distribution with $\\nu=1$')\n",
        "plt.ylabel('probability density')\n",
        "plot_x = np.linspace(-5, 5, 500)\n",
        "plt.plot(plot_x, sts.t.pdf(plot_x, df=1, loc=0, scale=1), label='T')\n",
        "plt.plot(plot_x, sts.norm.pdf(plot_x, loc=0, scale=1), label='Normal(0,1)')\n",
        "plt.legend()\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(0, plt.ylim()[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3416cd86",
      "metadata": {
        "id": "3416cd86"
      },
      "source": [
        "## The Student T model\n",
        "\n",
        "Modified likelihood:\n",
        "\n",
        "$$\\color{blue}{y_i \\sim \\text{T}(\\nu, \\mu_i, \\sigma)}$$\n",
        "\n",
        "Everything else stays the same, except that we require a prior for $\\nu$.\n",
        "\n",
        "$$\\mu_i = c_0 + c_1 x_i$$\n",
        "\n",
        "Prior:\n",
        "\n",
        "$$c_0 \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "$$c_1 \\sim \\text{Normal}(0, 5^2)$$\n",
        "\n",
        "$$\\sigma \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "$$\\color{blue}{\\nu \\sim \\text{Half-Normal}(\\sigma=30)}$$\n",
        "\n",
        "The prior over $\\nu$ is somewhat arbitrary. It is the positive half of a $\\text{Normal}(0, 30^2)$ distribution (since $\\nu>0$) with the scale parameter set to 30 so the prior allows for values of $\\nu$ that make the T distribution look like the Normal distribution. In this way, the model can adapt to whether or not there are outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2371613c",
      "metadata": {
        "id": "2371613c"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as t_model:\n",
        "    # Prior\n",
        "    c0 = pm.Uniform('c0', lower=0, upper=100)\n",
        "    c1 = pm.Normal('c1', mu=0, sigma=5)\n",
        "    sigma = pm.Uniform('sigma', lower=0, upper=100)\n",
        "    nu = pm.HalfNormal('nu', sigma=30)  # <== THIS LINE IS NEW\n",
        "    # Data\n",
        "    x = pm.Data('x', data_x)\n",
        "    y = pm.Data('y', data_y)\n",
        "    # Regression mean\n",
        "    mu = pm.Deterministic('mu', c0 + c1 * x)\n",
        "    # Likelihood\n",
        "    pm.StudentT('likelihood', nu=nu,  # <== THIS LINE IS DIFFERENT\n",
        "                mu=mu, sigma=sigma, observed=y)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(pm.model_to_graphviz(t_model).render(format='png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacceefc",
      "metadata": {
        "id": "eacceefc"
      },
      "source": [
        "If there are no outliers in the data, what do you expect the posterior distribution over $\\nu$ to look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89e71bdc",
      "metadata": {
        "id": "89e71bdc"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b39c2d",
      "metadata": {
        "id": "68b39c2d"
      },
      "source": [
        "### Student T model: Posteriors with all data\n",
        "\n",
        "We compute posteriors for the full data set and then for the data set excluding the outlier, to see how much the outlier affects the results.\n",
        "\n",
        "**Run this code cell** and check the diagnostic output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1363c777",
      "metadata": {
        "id": "1363c777"
      },
      "outputs": [],
      "source": [
        "with t_model:\n",
        "    t_inference_all = pm.sample()\n",
        "\n",
        "az.plot_rank(t_inference_all, var_names=['c0', 'c1', 'sigma', 'nu'])\n",
        "az.summary(t_inference_all, var_names=['c0', 'c1', 'sigma', 'nu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054c3ebc",
      "metadata": {
        "id": "054c3ebc"
      },
      "source": [
        "**Run this code cell** to make a pair plot of the posterior distribution over $(c_0, c_1, \\sigma, \\nu)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247ec025",
      "metadata": {
        "id": "247ec025"
      },
      "outputs": [],
      "source": [
        "ax = az.plot_pair(\n",
        "    t_inference_all,\n",
        "    var_names=['c0', 'c1', 'sigma', 'nu'],\n",
        "    marginals=True,\n",
        "    kind=[\"scatter\", \"kde\"],\n",
        "    scatter_kwargs={\"color\": \"C0\", \"alpha\": 0.05},\n",
        "    marginal_kwargs={\"kind\": \"kde\", \"color\": \"C0\"},\n",
        "    kde_kwargs={\"contour_kwargs\": {\"colors\": \"k\", 'alpha': 1}});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a498f13",
      "metadata": {
        "id": "5a498f13"
      },
      "source": [
        "This is a somewhat complicated 4-dimensional posterior distribution. You should spend some time looking at each pair plot to work out how the variables interact in the posterior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae146970",
      "metadata": {
        "id": "ae146970"
      },
      "source": [
        "As with the Normal likelihood model, we see a strong negative correlation between $c_0$ and $c_1$.\n",
        "\n",
        "Briefly explain why this negative correlation exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d31316",
      "metadata": {
        "id": "90d31316"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85e9c57",
      "metadata": {
        "id": "a85e9c57"
      },
      "source": [
        "There don't seem to be strong interactions (correlations) between $\\sigma$ and either $c_0$ or $c_1$.\n",
        "\n",
        "An interesting (and new) feature of the posterior distribution is the interaction between $\\nu$ and the other variables. Look at the $\\sigma$-$\\nu$ plot (last row, third column), for example.\n",
        "* If $\\nu$ is small (close to 0), the most likely values for $\\sigma$ are around $2$.\n",
        "* If $\\nu$ is large (greater than 20 or so), the possible $\\sigma$ values are quite spread out but somewhere in the range $[2.3, 3.1]$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63a50be3",
      "metadata": {
        "id": "63a50be3"
      },
      "source": [
        "Why, according to the posterior, does $\\sigma$ need to be larger when $\\nu$ is larger? Consider how the T distribution in the likelihood behaves for different values of $\\nu$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de83042e",
      "metadata": {
        "id": "de83042e"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "185d0da6",
      "metadata": {
        "id": "185d0da6"
      },
      "source": [
        "### Student T model: Posteriors without the outlier\n",
        "\n",
        "We recompute the posterior distribution with the same model and the data set that excludes the outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c53fe4",
      "metadata": {
        "id": "31c53fe4"
      },
      "outputs": [],
      "source": [
        "with t_model:\n",
        "    pm.set_data({\n",
        "        'x': data_removed_x,\n",
        "        'y': data_removed_y})\n",
        "    t_inference_without = pm.sample()\n",
        "\n",
        "az.plot_rank(t_inference_without, var_names=['c0', 'c1', 'sigma', 'nu'])\n",
        "az.summary(t_inference_without, var_names=['c0', 'c1', 'sigma', 'nu'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6e40dc",
      "metadata": {
        "id": "8e6e40dc"
      },
      "outputs": [],
      "source": [
        "ax = az.plot_pair(\n",
        "    t_inference_without,\n",
        "    var_names=['c0', 'c1', 'sigma', 'nu'],\n",
        "    marginals=True,\n",
        "    kind=[\"scatter\", \"kde\"],\n",
        "    scatter_kwargs={\"color\": \"C0\", \"alpha\": 0.05},\n",
        "    marginal_kwargs={\"kind\": \"kde\", \"color\": \"C0\"},\n",
        "    kde_kwargs={\"contour_kwargs\": {\"colors\": \"k\", 'alpha': 1}});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41ec394",
      "metadata": {
        "id": "d41ec394"
      },
      "source": [
        "### The difference\n",
        "\n",
        "**Run the code cell below** and interpret the results on the plots to see how the posterior over `c0`, `c1`, `sigma`, and `nu` changes with/without the outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a502941",
      "metadata": {
        "id": "2a502941"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.suptitle('Posterior distributions for the two data sets')\n",
        "for i, var in enumerate(['c0', 'c1', 'sigma', 'nu']):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    plt.xlabel(var)\n",
        "    if i == 0:\n",
        "        plt.ylabel('probability density')\n",
        "    plt.hist(getattr(t_inference_all.posterior, var).values.flatten(), density=True, bins=50, edgecolor='white', alpha=0.5, label='all data')\n",
        "    plt.hist(getattr(t_inference_without.posterior, var).values.flatten(), density=True, bins=50, edgecolor='white', alpha=0.5, label='no outlier')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f89d28",
      "metadata": {
        "id": "12f89d28"
      },
      "source": [
        "You should see that the posteriors over $c_0$, $c_1$, and $\\sigma$ for the two data sets are closer together than before. (Compare this plot to the matching plot for the Normal likelihood function.) However, there are obviously still some differences.\n",
        "\n",
        "So, the results are _more_ robust to outliers even though they are not perfectly robust. The outlier still makes a difference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9956c839",
      "metadata": {
        "id": "9956c839"
      },
      "source": [
        "Interpret the plot for $\\nu$ (on the far right). Explain how and why the posterior histograms differ for the two data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dff1ea7",
      "metadata": {
        "id": "9dff1ea7"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER IN YOUR FORUM PRE-CLASS WORKBOOK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b98252",
      "metadata": {
        "id": "61b98252"
      },
      "source": [
        "## Model 3: Outlier classifier\n",
        "\n",
        "This model is largely based on Hogg, D.W., Bovy, J., Lang, D. (2010). [Data analysis recipes: Fitting a model to data](https://arxiv.org/abs/1008.4686). arXiv:1008.4686.\n",
        "\n",
        "In the previous two models, we used either a light-tailed or a heavy-tailed model for _all_ the values in the data set. We assumed that either all values were generated from a light-tailed Normal distribution or all values were generated from a heavy-tailed Student T distribution.\n",
        "\n",
        "This assumption often doesn't align with what we really believe, namely that _some_ of the values come from a particular data distribution while _a few_ values are outliers that were generated in some other way — as a result of faulty measurement, for example.\n",
        "\n",
        "The next model encodes this assumption by assigning a binary variable to each data point with the value 1 if the data point is an outlier and 0 if it is not. This 0/1 value is generated from a Bernoulli distribution with probability $p$. We do not assume a particular value for $p$ here and infer it from the data.\n",
        "\n",
        "The likelihood is the same as the Normal model except that data points can have different variances (note the $i$ subscript of the $\\sigma$ variable).\n",
        "\n",
        "$$y_i \\sim \\text{Normal}(\\mu_i, \\sigma_i)$$\n",
        "\n",
        "$$\\mu_i = c_0 + c_1 x_i$$\n",
        "\n",
        "$$\\sigma_i = \\left\\{\\begin{array}{ll}\\sigma_{\\text{in}} & \\text{if } q_i = 0\\\\\\sigma_{\\text{in}}+\\sigma_{\\text{out}} & \\text{if } q_i = 1\\end{array}\\right.$$\n",
        "\n",
        "$$q_i \\sim \\text{Bernoulli}(p)$$\n",
        "\n",
        "Prior:\n",
        "\n",
        "$$c_0 \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "$$c_1 \\sim \\text{Normal}(0, 5^2)$$\n",
        "\n",
        "$$\\sigma_{\\text{in}} \\sim \\text{Uniform}(0, 100)$$\n",
        "\n",
        "$$\\sigma_{\\text{out}} \\sim \\text{Half-Normal}(0, 30^2)$$\n",
        "\n",
        "$$p \\sim \\text{Uniform}(0, ½)$$\n",
        "\n",
        "**Run this code cell** to create the model in PyMC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1a745d",
      "metadata": {
        "id": "ea1a745d"
      },
      "outputs": [],
      "source": [
        "import pytensor.tensor as pt\n",
        "\n",
        "with pm.Model() as outlier_model:\n",
        "\n",
        "    # Observed variables\n",
        "    x = pm.Data('x', data_x)\n",
        "    y = pm.Data('y', data_y)\n",
        "\n",
        "    # Linear regression\n",
        "    c0 = pm.Uniform('c0', lower=0, upper=100)\n",
        "    c1 = pm.Normal('c1', mu=0, sigma=5)\n",
        "    mu = pm.Deterministic('mu', c0 + c1 * x)\n",
        "\n",
        "    # Noise parameters for inliers and outliers\n",
        "    sigma = pm.Uniform('sigma', lower=0, upper=100)\n",
        "    sigma_out = pm.HalfNormal('sigma_out', sigma=30)\n",
        "    sigmas = pt.as_tensor_variable([sigma, sigma + sigma_out])\n",
        "\n",
        "    # In/out class assignment probability and indicators\n",
        "    p = pm.Uniform('p', lower=0, upper=0.5)\n",
        "    is_outlier = pm.Bernoulli('is_outlier', p=p, size=x.shape[0])\n",
        "\n",
        "    pm.Normal('likelihood', mu=mu, sigma=sigmas[is_outlier], observed=y)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(pm.model_to_graphviz(outlier_model).render(format='png'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with outlier_model:\n",
        "    # This runs the sampler to draw from the posterior\n",
        "    trace = pm.sample(2000, tune=1000, cores=1)\n",
        "\n",
        "# This creates a summary table of the posterior distribution\n",
        "summary = az.summary(trace, var_names=[\"c0\", \"c1\", \"sigma\", \"sigma_out\", \"p\"])\n",
        "\n",
        "# Print the summary table\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "tlFp6yF_g5aH"
      },
      "id": "tlFp6yF_g5aH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ccde5817",
      "metadata": {
        "id": "ccde5817"
      },
      "source": [
        "That's quite a complicated-looking model! Here are some important features to note.\n",
        "* The $p$, $c_0$, $c_1$, $\\sigma_{\\text{in}}$, and $\\sigma_{\\text{out}}$ variables are _outside_ the plate (the rectangular box). This means there is only one copy of each of those variables. In the mathematical description of the model, these variables do not have an $i$ subscript.\n",
        "* The $\\mu_i$, $q_i$ (`is_outlier`), $x_i$, and $y_i$ variables are inside the plate and there are 54 (the number of data) copies of them.\n",
        "* We have more unknown parameters than we have data! There are only 54 data but we have 54 (the $q_i$) + 5 ($p$, $c_0$, $c_1$, $\\sigma_{\\text{in}}$, and $\\sigma_{\\text{out}}$) = 59 free variables. Note that the $\\mu_i$ are not free variables since they depend deterministically on their inputs. It might seem that a model with more parameters than data is certain to overfit but this is not the case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "040eaf4a",
      "metadata": {
        "id": "040eaf4a"
      },
      "source": [
        "**Final task:** review the outlier classifier model above and make sure you understand every part of it. If something doesn't make sense, discuss it with your classmates and bring record any unresolved questions at the start of the notebook so we can discuss them in class.\n",
        "\n",
        "We compute and discuss the posteriors for this model in class. (Feel free to compute them yourself before class if you feel like it!)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}